<!DOCTYPE html><html lang="en" > <!-- The Head v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Improving 3D Human-Object Spatial Arrangement Predictions using Affordance -- Project Ideas | Jia Jiunn Ang</title><meta name="generator" content="Jekyll v4.1.1" /><meta property="og:title" content="Improving 3D Human-Object Spatial Arrangement Predictions using Affordance – Project Ideas" /><meta name="author" content="Jia Jiunn Ang" /><meta property="og:locale" content="en_US" /><meta name="description" content="Recently I’ve been quite intrigued by the area of pose estimation, and read quite a few papers on the subject. I thought I’d take this opportunity to share some of what I learned. I was particularly amazed by this paper, PHOSA, which proposes a novel framework for estimating human object spatial arrangements at the same time." /><meta property="og:description" content="Recently I’ve been quite intrigued by the area of pose estimation, and read quite a few papers on the subject. I thought I’d take this opportunity to share some of what I learned. I was particularly amazed by this paper, PHOSA, which proposes a novel framework for estimating human object spatial arrangements at the same time." /><link rel="canonical" href="https://jiajiunn.github.io/posts/phosa/" /><meta property="og:url" content="https://jiajiunn.github.io/posts/phosa/" /><meta property="og:site_name" content="Jia Jiunn Ang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-09-22T21:08:00-04:00" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"headline":"Improving 3D Human-Object Spatial Arrangement Predictions using Affordance – Project Ideas","dateModified":"2020-09-22T21:08:00-04:00","datePublished":"2020-09-22T21:08:00-04:00","description":"Recently I’ve been quite intrigued by the area of pose estimation, and read quite a few papers on the subject. I thought I’d take this opportunity to share some of what I learned. I was particularly amazed by this paper, PHOSA, which proposes a novel framework for estimating human object spatial arrangements at the same time.","mainEntityOfPage":{"@type":"WebPage","@id":"https://jiajiunn.github.io/posts/phosa/"},"@type":"BlogPosting","url":"https://jiajiunn.github.io/posts/phosa/","author":{"@type":"Person","name":"Jia Jiunn Ang"},"@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> <!-- CSS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --><link rel="preload" as="style" href="/assets/css/post.css"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /><link rel="preload" as="script" href="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" async></script> <!-- JS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --> <script src="/assets/js/post.min.js" async></script> <script src="/app.js" defer></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/sample/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/">Jia Jiunn Ang</a></div><div class="site-subtitle font-italic">Cornell University junior<br>studying CS & Statistics.</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></li></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <span id="mode-toggle-wrapper"> <!-- Switch the mode between dark and light. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <i class="mode-toggle fas fa-sun" dark-mode-invisible></i> <i class="mode-toggle fas fa-moon" light-mode-invisible></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.mode != null) { if (this.mode == ModeToggle.DARK_MODE) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.mode != null) { if (self.mode == ModeToggle.DARK_MODE) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightkMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightkMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span> <span class="icon-border"></span> <a href=" javascript:window.open('mailto:' + ['ja497','cornell.edu'].join('@'))" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/angjiajiunn/" target="_blank"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.facebook.com/angjiajiunn/" target="_blank"> <i class="fab fa-facebook-f"></i> </a> <a href="https://github.com/JiaJiunn" target="_blank"> <i class="fab fa-github-alt"></i> </a></div></div><!-- The Top Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Improving 3D Human-Object Spatial Arrangement Predictions using Affordance -- Project Ideas</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"> <!-- Refactor the HTML structure. --> <!-- Suroundding the markdown table with '<div class="table-wrapper">. and '</div>' --> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Improving 3D Human-Object Spatial Arrangement Predictions using Affordance -- Project Ideas</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Sep 22, 2020, 9:08 PM -0400" > Sep 22 <i class="unloaded">2020-09-22T21:08:00-04:00</i> </span> by <span class="author"> Jia Jiunn Ang </span></div><div> Updated <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Tue, Sep 22, 2020, 10:15 PM -0400" > Sep 22 <i class="unloaded">2020-09-22T22:15:42-04:00</i> </span></div></div><div class="post-content"><p>Recently I’ve been quite intrigued by the area of pose estimation, and read quite a few papers on the subject. I thought I’d take this opportunity to share some of what I learned. I was particularly amazed by <a href="https://arxiv.org/pdf/2007.15649.pdf">this</a> paper, PHOSA, which proposes a novel framework for estimating human object spatial arrangements at the same time.</p><p>Based on this, I also had a few project ideas I thought would be fun to attempt in order to improve this pipeline. Of course, I’m still quite new to the entire field, so my understanding is rather high-level and might not work at all — but I think it’ll be a great learning process over the coming few months, and I’ll try to document it here anyways.</p><p>Let’s start by first understanding what the paper is about!</p><h2 id="human-pose-estimation">Human Pose Estimation</h2><p>For the past few years, lots of work has been done on human pose estimation - given single images from monocular cameras, we are able to quite decently estimate the 3D pose of humans within the images. One of the more popular frameworks is from <a href="https://arxiv.org/pdf/1712.06584.pdf">this</a> paper on human mesh recovery, proposed by Kanazawa et al.; it first runs the image through 3D feature extractors and then pipes the output into a discriminator, which discriminates whether the predicted pose is logically possible for a normal human being. Here’s a visual from the paper:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/hmr_network.png" alt="hmr network" /></p><p>It’s purpose is to filter out poses that may although seem to have plausible predictions from the camera’s point of view, are actually clearly impossible for humans to achieve, such as those below:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/hmr_1.png" alt="hmr 1" /></p><p>Joo et al. then expanded upon this idea by including 3D priors of human poses into the pipeline:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/hmr_2.png" alt="hmr 2" /></p><p>I haven’t really looked through these papers in detail, but it would definitely be interesting to attempt implementing them in a future post.</p><h2 id="human-object-pose-estimation">Human-Object Pose Estimation</h2><p>But, to quote PHOSA’s authors, humans aren’t the end of the story. Estimating human poses might be interesting, but it would certainly be more useful in a practical sense to predict these human meshes in the context of its surroundings as well. Such a system could have a wide range of applications, from obtaining visual cues to action monitoring to trajectory predictions in autonomous cars. A problem we face is that most current approaches consider human and object pose estimation separately — and, when integrated separately into a global frame, we get figures like these which do not make sense spatially:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/phosa_before.png" alt="phosa before" /></p><p>Note: all images in the following sections are by PHOSA’s authors :)</p><p>To counter this problem, we refer to PHOSA, which refines this global pose estimation by taking into account the human object overlaps, typical object and human sizes, as well as typical interactions between humans and different object types. We’d ideally be able to refine the above image into:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/phosa_after.png" alt="phosa after" /></p><p>But before we delve into this correction process, let’s have a look at the object pose estimator PHOSA uses!</p><h2 id="object-estimation">Object Estimation</h2><p>Object estimation has also come a long way in the past decade. In particular, the object pose estimator that PHOSA’s authors implemented is as follows: given an image, we first segment objects in the image, using frameworks such as Mask R-CNN. We then an instance mask as well as an object class; the object class is then used to select an exemplar mask, which would be fitted onto the instance mask via an iterative process called a 3D differential renderer. In essence, it optimizes the 6 degree of freedoms for the mesh to best match the instance mask produced. Additionally, certain objects have multiple meshes to account for variations within the object class itself — for instance, there might be only one mesh for skateboards, but multiple 3D meshes for bikes, since the size and design can vary a lot. Here’s a really great illustration by PHOSA’s authors describing the process:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/obj_pose.gif" alt="obj pose" /></p><p>Authors of the paper also note that their implementation uses an occlusion-aware silhouette loss, which is robust to occlusion on the estimated object — as long as they have masks of the object that occludes it. More details can be found in <a href="https://arxiv.org/pdf/1912.08193.pdf">this</a> PointRend paper by Kirillov et al.; I definitely plan to explore this area more in future posts too.</p><h2 id="phosa">PHOSA</h2><p>So far, we have managed to produce 3D pose estimates of humans and objects separately.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/phosa_2.png" alt="phosa 2" /></p><p>The PHOSA paper then combines these images into a global coordinate frame, which they call “independent composition” – but due to depth ambiguity in single images, here’s the typical result:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/phosa_before_2.png" alt="phosa before 2" /></p><p>where again, the independent compositions don’t take into account how human-object pairs interact, leading to flawed combined results. This is what w want, though:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/phosa_after_2.png" alt="phosa after 2" /></p><p>But here’s where the fun starts! We now want to refine this estimation using some real world intuition and common sense. PHOSA, as I understand, is composed of two parts:</p><ul><li>Contact region enforcing</li><li>Scale correction</li></ul><h3 id="contact-region-enforcing">Contact Region Enforcing</h3><p>For contact region enforcing, the pipeline labels and classifies different contact regions on the human body, and labels the corresponding contact regions on all objects they have in the dataset. For instance, a human hand would typically be in contact with a bat handle, or the butt would typically touch the surface of a seat, as shown below:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/contact_1.png" alt="contact 1" /></p><p>The authors also included a component called the human-object interaction loss in their loss function, which optimizes for maximizing bounding box overlaps between contact regions. Here’s a visual from the paper:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/contact_2.png" alt="contact 2" /></p><p>In practice, the loss also solves the human-object overlap problem we noted previously, as it discourages deviations of the contact regions.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/compare.png" alt="compare" /></p><h3 id="scale-correction">Scale Correction</h3><p>Another intuition for human-object pose estimation we humans have is an estimate of how large an object typically is. The paper gives quite a nice illustrative example below:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/scale_1.png" alt="scale 1" /></p><p>Given the single image above, we would expect the surfer to be standing on the surf board, not just based on how the human-object combination typically interact, but also how large we expect a typical surf board to be. Notice that the first and second concepts both correspond to the two integral ideas behind PHOSA.</p><p>To encode this idea, authors of the paper translate all objects in the image into an intrinsic scale relative to a human of height 1.7 meters. Using this scale, they then initialized all object classes to their typical sizes as found on the internet. The results can be seen as such:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/scale_2.png" alt="scale 2" /></p><p>After training, here are what the authors found on the distribution of object sizes within their dataset:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/scale_results.png" alt="scale results" /></p><h3 id="network-and-loss">Network and Loss</h3><p>We now have the following pipeline:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/phosa_network.png" alt="phosa network" /></p><p>To wrap things up, here’s the loss function PHOSA uses:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/phosa_loss.png" alt="phosa loss" /></p><p>We can see that there’s the occlusion-aware silhouette loss used by the object pose estimator; the human-object interaction loss for enforcing contact regions; the scale prior for taking into account variations in object and human sizes (not everyone is 1.7 meters tall unfortunately); ordinal depth loss for refining object order from the front of the image; and interpenetration loss to discourage human-object overlaps.</p><h2 id="improvement-ideas">Improvement Ideas</h2><p>From my perspective, PHOSA can be improved in several areas; firstly, its contact region enforcing is rather loose, as it simply defines <em>where</em> the human-object pairs interact, rather than <em>how</em>. I thought I could somehow integrate the concept of affordances from robotics into the pipeline somehow, using ideas from a paper I’ll introduce later. Next, in terms of scale correction, large variances of size within the object class could potentially mess up the human-object interaction refinement. For instance, the car class could include anything ranging from small Mini Coopers to large SUVs; and when initialized incorrectly, could potentially mess up the assumed interaction completely. Of course, this problem can be easily fixed through using more fine-grained classes. As for the occlusion-aware silhouette loss, we seem to need masks for all object classes in the image, which would be tough for “in the wild” situations, which is the context of this paper. Lastly, the pipeline seems to be very far from real time, as the object pose estimation process itself takes minutes to complete.</p><h2 id="detecting-and-recognizing-human-object-interactions">Detecting and Recognizing Human-Object Interactions</h2><p>Now, on to the proposed solution. We first introduce <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Gkioxari_Detecting_and_Recognizing_CVPR_2018_paper.pdf">this</a> paper by Gkioxari et al., on detecting and recognizing human object interactions. Again, I’ll cover it in more detail in a future post; but on a surface level, this paper proposes a network that detects <code class="language-plaintext highlighter-rouge">&lt;human, verb, object&gt;</code> triplets from singular images, thereby inferring action and interactions within the image.</p><p>It builds off Faster R-CNN. Much like how Mask R-CNN simply adds a binary mask branch, this network adds two additional branches; the first one is a human-centric branch, which is given all the detected humans within the image. It focuses on classifying actions from those individuals, as well as predicting a target object density as a 4-dimensional Gaussian, where the mean is where we’d expect a target object to be. Examples of this can be seen below, from images I referenced from the paper:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/obj_pred.png" alt="obj pred" /></p><p>The red overlays are the inferred Gaussians of possible object locations based on the human pose. For instance, we would expect the man with arms outstretched and eyes forward to be reaching out for an object exactly at the position the red overlay indicates; that is the overall idea the first branch’s outputs.s</p><p>It is also important to note that a person can be doing multiple actions at once (reading and sipping tea, for instance). Hence, the network’s action (and score) output is fed into a binary sigmoid classifier to enable multiple predictions at the same time. Using this output, the second branch then scores the predicted actions based on human-object pairs in the image; it is thus called the interaction recognition branch.</p><h3 id="loss">Loss</h3><p>We use a loss comprising of 3 components:</p><ul><li>Classification and regression loss from object detection (Faster R-CNN)</li><li>Action classification and target localization loss from human-centric branch</li><li>Action classification loss from interaction recognition branch</li></ul><p>On an unrelated note, the paper measures precision and recall by considering a classification as a true positive only when the human and object detected have an IOU of greater than 0.5, as well as a matching action labelled for the pair.</p><h2 id="bringing-it-all-together">Bringing It All Together</h2><p>Now, back to addressing problems above. We want a pipeline that is ideally simpler than PHOSA’s proposed network; it is currently comprised of three stages: generating human and object pose estimations independently, combining them in a common coordinate frame, then refining the combination. Much like the transition from R-CNN to Fast R-CNN, we want to simplify this network into as few stages as possible, which would potentially make optimizing the pipeline easier in the future.</p><h3 id="project-idea">Project Idea</h3><p>Well, we first look at the differential renderer — why are we optimizing the object mask to match the instance mask? The instance mask is but an arbitrary pose estimation in the shared coordinate space; when we eventually combine the human and object poses together, there is still an entire degree of freedom which it can lie anywhere on. Intuitively, I thought that instead of optimizing the pose search on this somewhat arbitrary positioning, we could utilize the 3D human pose to predict potential object locations within the global coordinate space, and then optimize localization to those positions. This would</p><ol><li>Create a simpler pipeline that is easier to optimize, since we don’t need to generate the object separately in a different coordinate frame then combine it back.</li><li>Utilizes object affordance and how objects interact rather than just where, which might help improve contact region enforcing.</li><li>Doesn’t need to account for occlusion masks of all possible classes, since the predicted pose would be in a separate 3D space.</li></ol><p>Here’s what I had in mind:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/phosa/proposal.png" alt="network proposal" /></p><p>Again, I might be totally wrong about this idea, as I don’t really have much background in the field — but it definitely looks really fun to try implementing this and see where it goes. Any feedback or advice would be really appreciated :) hopefully I’ll give some good updates in the next post!</p></div><div class="post-tail-wrapper text-muted"><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/pose-estimation/" class="post-tag no-text-decoration" >pose-estimation</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><!-- Post sharing snippet v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.facebook.com/sharer/sharer.php?title=Improving 3D Human-Object Spatial Arrangement Predictions using Affordance -- Project Ideas - Jia Jiunn Ang&u=https://jiajiunn.github.io/posts/phosa/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://twitter.com/intent/tweet?text=Improving 3D Human-Object Spatial Arrangement Predictions using Affordance -- Project Ideas - Jia Jiunn Ang&url=https://jiajiunn.github.io/posts/phosa/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://telegram.me/share?text=Improving 3D Human-Object Spatial Arrangement Predictions using Affordance -- Project Ideas - Jia Jiunn Ang&url=https://jiajiunn.github.io/posts/phosa/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><!-- The Pannel on right side (Desktop views) v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/phosa/">Improving 3D Human-Object Spatial Arrangement Predictions using Affordance -- Project Ideas</a></li><li><a href="/posts/datasets/">Tensorflow and Pytorch Torchvision Datasets</a></li><li><a href="/posts/pipeline/">Software Structure for a Machine Learning Pipeline in PyTorch</a></li><li><a href="/posts/yolo/">Implementing YOLOv3 in Tensorflow</a></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/mask-rcnn/">mask rcnn</a> <a class="post-tag" href="/tags/yolo/">yolo</a> <a class="post-tag" href="/tags/pose-estimation/">pose estimation</a> <a class="post-tag" href="/tags/pipeline/">pipeline</a> <a class="post-tag" href="/tags/datasets/">datasets</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><h3 data-toc-skip class="pl-3 pt-2 mb-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <!-- Recommend the other 3 posts according to the tags and categories of the current post, if the number is not enough, use the other latest posts to supplement. v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/datasets/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Sep 13 <i class="unloaded">2020-09-13T00:20:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Tensorflow and Pytorch Torchvision Datasets</h3><div class="text-muted small"><p> An integral component of any high-performing model is the dataset; it defines the real world examples that form the basis of what a model learns. But for some, gathering a clean, well-formatted dat...</p></div></div></a></div><div class="card"> <a href="/posts/pipeline/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Sep 11 <i class="unloaded">2020-09-11T02:01:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Software Structure for a Machine Learning Pipeline in PyTorch</h3><div class="text-muted small"><p> Recently, I’ve been working on designing an end-to-end machine learning model pipeline for a project I am involved in. I just thought I’d share some of my high-level designs here, as a way of summa...</p></div></div></a></div><div class="card"> <a href="/posts/mask-rcnn-1/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Aug 30 <i class="unloaded">2020-08-30T21:35:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Mask R-CNN in Keras, Part 1&#58; Backbone and RPN</h3><div class="text-muted small"><p> In my previous post, I went through a somewhat high level overview of the R-CNN family, which personally gave me a fairly intuitive idea of each of Mask R-CNN’s components. Nevertheless, to get a d...</p></div></div></a></div></div></div><!-- Navigation buttons at the bottom of the post. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --><div class="post-navigation d-flex justify-content-between"> <a href="/posts/datasets/" class="btn btn-outline-primary"><p>Tensorflow and Pytorch Torchvision Datasets</p></a> <span class="btn btn-outline-primary disabled"><p>-</p></span></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script> <!-- The Footer v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2020 <a href="https://github.com/JiaJiunn">Jia Jiunn Ang</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a> theme.</p></div></div></footer></div><!-- The Search results v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/mask-rcnn/">mask rcnn</a> <a class="post-tag" href="/tags/yolo/">yolo</a> <a class="post-tag" href="/tags/pose-estimation/">pose estimation</a> <a class="post-tag" href="/tags/pipeline/">pipeline</a> <a class="post-tag" href="/tags/datasets/">datasets</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <!-- The GA snippet v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script> <!-- Jekyll Simple Search loader v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://jiajiunn.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
