<!DOCTYPE html><html lang="en" > <!-- The Head v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Mask R-CNN in Keras, Part 1&#58; Backbone and RPN | Jia Jiunn Ang</title><meta name="generator" content="Jekyll v4.1.1" /><meta property="og:title" content="Mask R-CNN in Keras, Part 1: Backbone and RPN" /><meta name="author" content="Jia Jiunn Ang" /><meta property="og:locale" content="en_US" /><meta name="description" content="In my previous post, I went through a somewhat high level overview of the R-CNN family, which personally gave me a fairly intuitive idea of each of Mask R-CNN’s components. Nevertheless, to get a deeper understanding on the network, I thought it’d be a fun project to go through an actual implementation of the paper itself." /><meta property="og:description" content="In my previous post, I went through a somewhat high level overview of the R-CNN family, which personally gave me a fairly intuitive idea of each of Mask R-CNN’s components. Nevertheless, to get a deeper understanding on the network, I thought it’d be a fun project to go through an actual implementation of the paper itself." /><link rel="canonical" href="https://jiajiunn.github.io/posts/mask-rcnn-1/" /><meta property="og:url" content="https://jiajiunn.github.io/posts/mask-rcnn-1/" /><meta property="og:site_name" content="Jia Jiunn Ang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-08-30T21:35:00-04:00" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"headline":"Mask R-CNN in Keras, Part 1: Backbone and RPN","dateModified":"2020-08-30T21:35:00-04:00","datePublished":"2020-08-30T21:35:00-04:00","description":"In my previous post, I went through a somewhat high level overview of the R-CNN family, which personally gave me a fairly intuitive idea of each of Mask R-CNN’s components. Nevertheless, to get a deeper understanding on the network, I thought it’d be a fun project to go through an actual implementation of the paper itself.","mainEntityOfPage":{"@type":"WebPage","@id":"https://jiajiunn.github.io/posts/mask-rcnn-1/"},"@type":"BlogPosting","url":"https://jiajiunn.github.io/posts/mask-rcnn-1/","author":{"@type":"Person","name":"Jia Jiunn Ang"},"@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> <!-- CSS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --><link rel="preload" as="style" href="/assets/css/post.css"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /><link rel="preload" as="script" href="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" async></script> <!-- JS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --> <script src="/assets/js/post.min.js" async></script> <script src="/app.js" defer></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/sample/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/">Jia Jiunn Ang</a></div><div class="site-subtitle font-italic">Cornell University junior<br>studying CS & Statistics.</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></li></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <span id="mode-toggle-wrapper"> <!-- Switch the mode between dark and light. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <i class="mode-toggle fas fa-sun" dark-mode-invisible></i> <i class="mode-toggle fas fa-moon" light-mode-invisible></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.mode != null) { if (this.mode == ModeToggle.DARK_MODE) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.mode != null) { if (self.mode == ModeToggle.DARK_MODE) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightkMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightkMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span> <span class="icon-border"></span> <a href=" javascript:window.open('mailto:' + ['ja497','cornell.edu'].join('@'))" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/angjiajiunn/" target="_blank"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.facebook.com/angjiajiunn/" target="_blank"> <i class="fab fa-facebook-f"></i> </a> <a href="https://github.com/JiaJiunn" target="_blank"> <i class="fab fa-github-alt"></i> </a></div></div><!-- The Top Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Mask R-CNN in Keras, Part 1&#58; Backbone and RPN</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"> <!-- Refactor the HTML structure. --> <!-- Suroundding the markdown table with '<div class="table-wrapper">. and '</div>' --> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Mask R-CNN in Keras, Part 1&#58; Backbone and RPN</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sun, Aug 30, 2020, 9:35 PM -0400" > Aug 30 <i class="unloaded">2020-08-30T21:35:00-04:00</i> </span> by <span class="author"> Jia Jiunn Ang </span></div></div><div class="post-content"><p>In my previous post, I went through a somewhat high level overview of the R-CNN family, which personally gave me a fairly intuitive idea of each of <a href="https://arxiv.org/pdf/1703.06870.pdf">Mask R-CNN</a>’s components. Nevertheless, to get a deeper understanding on the network, I thought it’d be a fun project to go through an actual implementation of the paper itself.</p><h2 id="foreword">Foreword</h2><p>What I had in mind was to write an actual implementation of Mask R-CNN from scratch, and be able to run it on example images and get the corresponding predicted masks, bounding boxes and class probabilities. This, however, poses a limitation, since I don’t really plan to set up a training pipeline and spend hours training a decent performing model — hence, I resorted to finding model weights from other implementations online, and tried not to deviate too much from their implementation architecture (i.e. if they use a ResNet101 backbone, I would need to follow suit).</p><p>In this post, I am using matterport’s publicly-available weights, whose implementation can be found <a href="https://github.com/matterport/Mask_RCNN">here</a>. I ended up referencing their implementation a lot, since I had to match the architecture up to their tensor names too, for the weights to be loaded in correctly. Nevertheless, in this post I will try to focus on explaining what I learned — since that is ultimately my goal — but do note that a fair bit of code is not from my own implementation ideas, but <a href="https://github.com/matterport/Mask_RCNN">matterport’s</a>.</p><h2 id="keras-implementation-and-loading-weights">Keras Implementation and Loading Weights</h2><p>To give an overview of how the Mask R-CNN model is structured, we will be using <a href="https://keras.io/api/models/model/">Keras’ Model API</a>. How the entire model and most of the component will be written is through an API like below:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>inputs = tf.keras.layers.Input(shape=…, name=“…”)
outputs = process_inputs(inputs)
model = tf.keras.models.Model(inputs, outputs, name=“…”) 
</pre></table></code></div></div><p>We can then use the model to run predictions by calling</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>image = cv2.imread(“…”)  # if the input is an image
outputs = model.predict(image)
</pre></table></code></div></div><p>But most importantly, how do we load in our weights? Well, matterport’s weights are saved in a H5 file format, so we can use the Keras API for parsing the file as such:</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre>import h5py
from tensorflow.python.keras.saving import hdf5_format
f = h5py.File(file_name, mode='r')
hdf5_format.load_weights_from_hdf5_group_by_name(f, model.layers)
</pre></table></code></div></div><p>where <code class="language-plaintext highlighter-rouge">model.layers</code> will give all the layers within the model by name, to be loaded with their corresponding weights. Now that we understand this, we will construct the entire Mask R-CNN architecture as a huge Keras Model object, and load in the weights later as in the example above. Let’s get started!</p><h2 id="resnetfpn-backbone">ResNet/FPN Backbone</h2><p>If you recall from the previous post, we first extract features from the input image with a CNN. For this purpose, two popular choices are ResNet and VGG, with ResNet seemingly being the preferred choice as it has skip connections and batch normalization components, which weren’t invented during VGG’s time. There are way more details <a href="https://stats.stackexchange.com/questions/280179/why-is-resnet-faster-than-vgg">here</a>, but we are going to implement a 101-layer <a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a> today since it is the architecture used by our stolen weights. In addition, from the <a href="https://arxiv.org/pdf/1703.06870.pdf">Mask R-CNN paper</a>, the authors found that using a feature pyramid network (FPN) was more effective in capturing features of different scales; hence, we are incorporating this at the end of our ResNet backbone too.</p><h3 id="resnet">ResNet</h3><p>ResNet is a single convolutional network. At its core, it is comprised of only two types of blocks: the basic block, which has two 3x3 convolutions, and the bottleneck block, which has a 1x1 convolution for dimension control, a 3x3 convolution, and a 1x1 convolution yet again as an upsampling layer. For a more concrete example, take a look at this figure from the <a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a> paper:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/mask_rcnn_1/resnet_arch.png" alt="ResNet arch" /></p><p>The layers wrapped in <code class="language-plaintext highlighter-rouge">[]</code>s are what I refer to as blocks. We can see that the basic block is used exclusively in ResNet18 and ResNet34, while the bottleneck block is used by all the larger ResNet variations. From what I read, one reason for this is that running convolutions on larger feature map inputs tend to be time-consuming, hence bottleneck blocks serve to reduce the size of feature maps before performing convolutions, then upsampling them again before returning the output.</p><p>Regardless of block type, though, the most important aspect of ResNets is what is known as the skip connection around each block. Before the introduction of ResNet, networks with large number of layers are discouraged, since this led to more multiplications during backprop, which usually reduce the gradients exponentially to 0. The introduction of skip connections by the ResNet paper serves to solve this through an identity function from the very start of each block, which is added to the block output; if the identity mapping is optimal, the network then can easily take the identity function and push the residual to 0, rather than forcefully fitting the identity mapping to a stack of non-linear convolution layers.</p><p>In summary, here’s what a block looks like in code:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/mask_rcnn_1/identity_block.png" alt="Identity block" /></p><p>where <code class="language-plaintext highlighter-rouge">input_tensor</code> is the identity mapping that “skips” to the end of the block.</p><p>Now, we are only missing one last concept about ResNets: downsampling. Notice that on the left side of the table above, there is a list of layer names. With the exception of the first and second layers, every other layer is preceded by a downsampling during the first convolution. That is to say, during the first block of each of <code class="language-plaintext highlighter-rouge">conv3_x</code>, <code class="language-plaintext highlighter-rouge">conv4_x</code>, and <code class="language-plaintext highlighter-rouge">conv5_x</code>, the convolution first convolution has a stride of 2 to correspond to a downsample. In response, the skip connection also should have a convolution with the same amount of stride, as follows.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/mask_rcnn_1/convolution_block.png" alt="Convolution block" /></p><p>Now, all we need to do is assemble the pieces together. For easy visualization, here’s also a screenshot of matterport’s implementation: you can see that aside from the first layer, each <code class="language-plaintext highlighter-rouge">conv_x</code> layer is preceded with a convolutional-type block before the identity-type blocks. Also notice how for the second layer’s convolution-type block, the stride is 1 instead of 2, as we mentioned above. We then output the feature maps from each convolution layer into our feature pyramid network.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/mask_rcnn_1/resnet_code.png" alt="ResNet code" /></p><h3 id="feature-pyramid-network">Feature Pyramid Network</h3><p>An FPN, as the name suggests, essentially forms a pyramid of feature maps at different scales. If you notice from the our modified ResNet before, we returned each convolution layer’s output as opposed to only the final layer’s. The intuition behind this is when we only choose one feature map, for instance the final feature map of our ResNet output, C5, we are committing ourselves to just one particular scale and resolution for our detected objects. At the scale of C5, we would miss out of plenty of smaller objects, since its resolution is really small.</p><p>However, the reason we conventionally take the C5 output is that as the number of convolutions increase, from C2 to C3 to C5, the semantic value learnedΩ in each feature map increases, which is what we look for in detecting objects and their classes. The initial feature maps like C2 and C3, although high in resolution, simply have not learned enough semantic meaning to justify using them for inference. To counter this problem, though, all we have to do is take the semantic value learned from the last feature map, and combine it with our higher-resolution feature maps, to capture the semantic value of smaller objects. Here’s a <a href="https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c">really good explanation</a> on FPNs and how they work in an RPN, from which I stole the following graphic:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/mask_rcnn_1/fpn_graphic.png" alt="FPN graphic" /></p><p>As you can see, by obtaining feature maps at various scales, adding semantic values, then running inference on them, we can then capture a larger variety of objects at different sizes. Here’s the implementation with corresponding variable names for better reference:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/mask_rcnn_1/fpn_implementation.png" alt="FPN implementation" /></p><h3 id="region-proposal-network">Region Proposal Network</h3><p>Finally, we take these feature maps of different resolutions, and use a CNN to predict regions with objects in them. Here’s a description of the outputs of the CNN, taken from the <a href="https://arxiv.org/abs/1506.01497">Faster R-CNN paper</a> where RPNs were introduced:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/mask_rcnn_1/rpn_graphic.png" alt="RPN graphic" /></p><p>where k is the number of anchor boxes. As I described in a previous post, anchor boxes are essentially aspect ratios of the width and height which we commonly expect objects to be close to. Typically, we use these three aspect ratios: <code class="language-plaintext highlighter-rouge">[0.5, 1, 2]</code>. On the left of the figure, the 2 numbers for each class prediction is the score of how likely that object is a foreground object and a background object respectively. On the right, the 4 numbers simply defines the bounding box of the object at that anchor. Below is the Python implementation:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/mask_rcnn_1/rpn_implementation.png" alt="RPN implementation" /></p><p>We take the softmax of the class score output to normalize them into the class probabilities. Notice that in the implementation above, by setting the stride of the shared convolution to the default value of 1, we are creating anchor boxes for each grid in the feature map. We can vary this too, for instance, by setting its stride to be 2, which creates anchor boxes at every other grid.</p><p>And now, we are done with the RPN stage! We have just generated a ton of bounding box predictions, along with the predicted foreground/background class probabilities for each bounding box. To test this out, I hacked together a script based on matterport’s <a href="https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/inspect_model.ipynb">model analysis code</a>, that displays our predicted bounding boxes. Essentially, after outputting the RPN class probabilities, I sorted them and found the 100 highest foreground class probabilities. I then found the corresponding bounding boxes and overlayed them onto the image.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/mask_rcnn_1/visualize_script.png" alt="Visualize script" /></p><p>And finally, here are some output images!</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/mask_rcnn_1//sample_output_1.png" alt="image_1" /> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/mask_rcnn_1/sample_output_2.png" alt="image_2" /> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/mask_rcnn_1/sample_output_3.png" alt="image_3" /></p><h2 id="next-stages">Next Stages</h2><p>Notice that the image has been transformed into a square format. Ideally, we would also crop the bounding boxes that have exceeded the image boundaries, as well as apply non-maximum suppression to reduce the number of overlapping bounding boxes. I’ll include that in a different post, though, as well as implementations and explanations for the rest of the network.</p><p>For now, I hope the first part of the model makes sense to you idea-wise – we now have a working framework for obtaining region proposals, which potentionally contain objects. All we need to do now is transform the regions into a standardized size, and feed it into (in parallel):</p><ol><li>a classifier for predicting object class</li><li>a second classifier for inferring binary masks (whether a pixel is part of an object or not)</li><li>a regressor for tightening the bounding box</li></ol></div><div class="post-tail-wrapper text-muted"><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/mask-rcnn/" class="post-tag no-text-decoration" >mask-rcnn</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><!-- Post sharing snippet v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.facebook.com/sharer/sharer.php?title=Mask R-CNN in Keras, Part 1&#58; Backbone and RPN - Jia Jiunn Ang&u=https://jiajiunn.github.io/posts/mask-rcnn-1/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://twitter.com/intent/tweet?text=Mask R-CNN in Keras, Part 1&#58; Backbone and RPN - Jia Jiunn Ang&url=https://jiajiunn.github.io/posts/mask-rcnn-1/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://telegram.me/share?text=Mask R-CNN in Keras, Part 1&#58; Backbone and RPN - Jia Jiunn Ang&url=https://jiajiunn.github.io/posts/mask-rcnn-1/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><!-- The Pannel on right side (Desktop views) v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/phosa/">Improving 3D Human-Object Spatial Arrangement Predictions using Affordance -- Project Ideas</a></li><li><a href="/posts/datasets/">Tensorflow and Pytorch Torchvision Datasets</a></li><li><a href="/posts/pipeline/">Software Structure for a Machine Learning Pipeline in PyTorch</a></li><li><a href="/posts/yolo/">Implementing YOLOv3 in Tensorflow</a></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/mask-rcnn/">mask rcnn</a> <a class="post-tag" href="/tags/yolo/">yolo</a> <a class="post-tag" href="/tags/pose-estimation/">pose estimation</a> <a class="post-tag" href="/tags/pipeline/">pipeline</a> <a class="post-tag" href="/tags/datasets/">datasets</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><h3 data-toc-skip class="pl-3 pt-2 mb-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <!-- Recommend the other 3 posts according to the tags and categories of the current post, if the number is not enough, use the other latest posts to supplement. v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/rcnn-fam/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Aug 29 <i class="unloaded">2020-08-29T13:09:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>A (Really) Brief Overview of the RCNN Family</h3><div class="text-muted small"><p> Lately I’ve been reading up on Mask-RCNN and figuring out how to best implement it, but in the process, I realized that going through the development history of the RCNN family actually gives a fai...</p></div></div></a></div><div class="card"> <a href="/posts/phosa/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Sep 22 <i class="unloaded">2020-09-22T21:08:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Improving 3D Human-Object Spatial Arrangement Predictions using Affordance -- Project Ideas</h3><div class="text-muted small"><p> Recently I’ve been quite intrigued by the area of pose estimation, and read quite a few papers on the subject. I thought I’d take this opportunity to share some of what I learned. I was particularl...</p></div></div></a></div><div class="card"> <a href="/posts/datasets/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Sep 13 <i class="unloaded">2020-09-13T00:20:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Tensorflow and Pytorch Torchvision Datasets</h3><div class="text-muted small"><p> An integral component of any high-performing model is the dataset; it defines the real world examples that form the basis of what a model learns. But for some, gathering a clean, well-formatted dat...</p></div></div></a></div></div></div><!-- Navigation buttons at the bottom of the post. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --><div class="post-navigation d-flex justify-content-between"> <a href="/posts/rcnn-fam/" class="btn btn-outline-primary"><p>A (Really) Brief Overview of the RCNN Family</p></a> <a href="/posts/pipeline/" class="btn btn-outline-primary"><p>Software Structure for a Machine Learning Pipeline in PyTorch</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script> <!-- The Footer v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2020 <a href="https://github.com/JiaJiunn">Jia Jiunn Ang</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a> theme.</p></div></div></footer></div><!-- The Search results v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/mask-rcnn/">mask rcnn</a> <a class="post-tag" href="/tags/yolo/">yolo</a> <a class="post-tag" href="/tags/pose-estimation/">pose estimation</a> <a class="post-tag" href="/tags/pipeline/">pipeline</a> <a class="post-tag" href="/tags/datasets/">datasets</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <!-- The GA snippet v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script> <!-- Jekyll Simple Search loader v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://jiajiunn.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
