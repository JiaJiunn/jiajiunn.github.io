<!DOCTYPE html><html lang="en" > <!-- The Head v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Implementing YOLOv3 in Tensorflow | Jia Jiunn Ang</title><meta name="generator" content="Jekyll v4.1.1" /><meta property="og:title" content="Implementing YOLOv3 in Tensorflow" /><meta name="author" content="Jia Jiunn Ang" /><meta property="og:locale" content="en_US" /><meta name="description" content="For quite some time now, I’ve been thinking of implementing several of the more iconic object detection models — such as YOLO, Mask-RCNN, etc. — from scratch. This is in part motivated by the fact that I haven’t actually taken a machine learning course formally; most of what I know about the field is through extracurriculars, work, and the occasional medium article, giving me a scattered understanding of concepts here and there. In retrospect, I wanted to start getting a more standardized understanding of the field, and implementing these models myself would hopefully lead to a deeper understanding of their architecture details and reasoning." /><meta property="og:description" content="For quite some time now, I’ve been thinking of implementing several of the more iconic object detection models — such as YOLO, Mask-RCNN, etc. — from scratch. This is in part motivated by the fact that I haven’t actually taken a machine learning course formally; most of what I know about the field is through extracurriculars, work, and the occasional medium article, giving me a scattered understanding of concepts here and there. In retrospect, I wanted to start getting a more standardized understanding of the field, and implementing these models myself would hopefully lead to a deeper understanding of their architecture details and reasoning." /><link rel="canonical" href="https://jiajiunn.github.io/posts/yolo/" /><meta property="og:url" content="https://jiajiunn.github.io/posts/yolo/" /><meta property="og:site_name" content="Jia Jiunn Ang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-08-27T01:45:00-04:00" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"headline":"Implementing YOLOv3 in Tensorflow","dateModified":"2020-08-27T01:45:00-04:00","datePublished":"2020-08-27T01:45:00-04:00","description":"For quite some time now, I’ve been thinking of implementing several of the more iconic object detection models — such as YOLO, Mask-RCNN, etc. — from scratch. This is in part motivated by the fact that I haven’t actually taken a machine learning course formally; most of what I know about the field is through extracurriculars, work, and the occasional medium article, giving me a scattered understanding of concepts here and there. In retrospect, I wanted to start getting a more standardized understanding of the field, and implementing these models myself would hopefully lead to a deeper understanding of their architecture details and reasoning.","mainEntityOfPage":{"@type":"WebPage","@id":"https://jiajiunn.github.io/posts/yolo/"},"@type":"BlogPosting","url":"https://jiajiunn.github.io/posts/yolo/","author":{"@type":"Person","name":"Jia Jiunn Ang"},"@context":"https://schema.org"}</script> <!-- The Favicons for Web, Android, Microsoft, and iOS (iPhone and iPad) Apps Generated by: https://www.favicon-generator.org/ v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT license --><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="preload" as="style" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"> <!-- CSS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --><link rel="preload" as="style" href="/assets/css/post.css"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /><link rel="preload" as="script" href="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"> <script src="https://cdn.jsdelivr.net/npm/jquery@3.4.1" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" async></script> <!-- JS selector for site. Chirpy v2.3 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT Licensed --> <script src="/assets/js/post.min.js" async></script> <script src="/app.js" defer></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"> <!-- The Side Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/sample/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/">Jia Jiunn Ang</a></div><div class="site-subtitle font-italic">Cornell University junior<br>studying CS & Statistics.</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a></li><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></li></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <span id="mode-toggle-wrapper"> <!-- Switch the mode between dark and light. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <i class="mode-toggle fas fa-sun" dark-mode-invisible></i> <i class="mode-toggle fas fa-moon" light-mode-invisible></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.mode != null) { if (this.mode == ModeToggle.DARK_MODE) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.mode != null) { if (self.mode == ModeToggle.DARK_MODE) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightkMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightkMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span> <span class="icon-border"></span> <a href=" javascript:window.open('mailto:' + ['ja497','cornell.edu'].join('@'))" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/angjiajiunn/" target="_blank"> <i class="fab fa-linkedin"></i> </a> <a href="https://www.facebook.com/angjiajiunn/" target="_blank"> <i class="fab fa-facebook-f"></i> </a> <a href="https://github.com/JiaJiunn" target="_blank"> <i class="fab fa-github-alt"></i> </a></div></div><!-- The Top Bar v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Implementing YOLOv3 in Tensorflow</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"> <!-- Refactor the HTML structure. --> <!-- Suroundding the markdown table with '<div class="table-wrapper">. and '</div>' --> <!-- Fixed kramdown code highlight rendering: https://github.com/penibelst/jekyll-compress-html/issues/101 https://github.com/penibelst/jekyll-compress-html/issues/71#issuecomment-188144901 --><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Implementing YOLOv3 in Tensorflow</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Aug 27, 2020, 1:45 AM -0400" > Aug 27 <i class="unloaded">2020-08-27T01:45:00-04:00</i> </span> by <span class="author"> Jia Jiunn Ang </span></div><div> Updated <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Thu, Aug 27, 2020, 2:22 PM -0400" > Aug 27 <i class="unloaded">2020-08-27T14:22:33-04:00</i> </span></div></div><div class="post-content"><p>For quite some time now, I’ve been thinking of implementing several of the more iconic object detection models — such as YOLO, Mask-RCNN, etc. — from scratch. This is in part motivated by the fact that I haven’t actually taken a machine learning course formally; most of what I know about the field is through extracurriculars, work, and the occasional medium article, giving me a scattered understanding of concepts here and there. In retrospect, I wanted to start getting a more standardized understanding of the field, and implementing these models myself would hopefully lead to a deeper understanding of their architecture details and reasoning.</p><p>Well, this week I thought I’d start off with implementing <a href="https://arxiv.org/pdf/1506.02640.pdf">YOLOv3</a>. Skimming through the original YOLO paper, YOLO seems fairly straightforward in comparison to other recent object recognition models. For instance, models from the R-CNN family, like R-CNN, Fast R-CNN, Faster R-CNN, and to a certain extent, Mask R-CNN, seem to approach the object detection problem by first generating potential bounding boxes, then running a classifier on these boxes.</p><p>In retrospect, YOLO approaches our object detection problem just like a single regression problem: it shoves the image into a single convolutional network and spits out the values we want in a particular format. In contrast to R-CNN’s somewhat two-stage pipeline, YOLO is both simpler implementation-wise, and also runs faster in practice, since the network is not as complicated. Authors of the YOLOv3 paper reported that they still get performance on par with other SOTA classifiers, but with significantly faster speed:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/model_stats.png" alt="Performance comparison from YOLOv3 paper" /></p><p>That being said, let’s get right down to implementing it!</p><h2 id="implementation">Implementation</h2><p>To start off, I took a look at this config file provided by Joseph Redmon, one of the authors of the paper, who linked it on his <a href="https://pjreddie.com/darknet/yolo/">official website</a>. You may remember him from a few years ago, when he was massively popular on reddit for his <a href="https://pjreddie.com/static/Redmon%20Resume.pdf">my little pony resume</a>. Anyhow, The config file essentially specifies YOLO as a sequential order of blocks, and also provide the parameters for each block. Much like <a href="https://github.com/ayooshkathuria/YOLO_v3_tutorial_from_scratch">this</a> PyTorch implementation, I thought that it’d make more sense to iterate through the config file in order to build the model, rather than manually code each block’s parameters in a huge mess of a file. This way, the code would not only be shorter and cleaner, but we can also have all the parameters specified in one place. After parsing the config file, here’s the general idea I had in mind:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/implementation_idea.png" alt="General idea" /></p><p>Looking through the configs, the first block, <code class="language-plaintext highlighter-rouge">net</code>, seems to specify the network input (such as batch size and image size) as well as training parameters, and isn’t really specifying a block type. Starting from the second block, however, all the blocks below each specify one of five different block types. We will go through each block type below.</p><h3 id="convolution-block">Convolution block</h3><p>The <code class="language-plaintext highlighter-rouge">convolutional</code> block is probably the most straightforward block. We simply parse the parameters from the config file into the Tensorflow convolution function. The only noteworthy I learned to look out for is that I find it somewhat awkward to write paddings for the input tensors; Tensorflow doesn’t support padding as one of the parameters for the in-built convolutional function, unlike PyTorch.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/conv.png" alt="Convolutional block" /></p><h3 id="upsample-block">Upsample block</h3><p>The next block type is <code class="language-plaintext highlighter-rouge">upsampling</code>, which is also rather straightforward to do in Tensorflow. We simply call the resize function below, and it seems like Tensorflow by default uses bilinear interpolation for its resizing.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/upsample.png" alt="Upsample block" /></p><h3 id="route-block">Route block</h3><p>Now, the <code class="language-plaintext highlighter-rouge">route</code> block is still straightforward, but might need some explaining. It has a parameter called layers which is either one or two numbers. The number represents the index of the layer that will be outputted by the route block. For instance, when the layer parameter has one number, -1, the route block output will the layer behind the route layer. When it has two numbers, say -1, 61, then the output will be the concatenation of the layer behind the route layer, together with the 61st layer (yes, so we need to take into account the absolute vs relative layer indices while parsing). Of course, this by itself isn’t complicated to implement; we then just set up a list <code class="language-plaintext highlighter-rouge">outputs</code> and keep track of all outputs by each layer.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/route.png" alt="Route block" /></p><h3 id="shortcut-block">Shortcut block</h3><p>And for the most straightforward block, the <code class="language-plaintext highlighter-rouge">shortcut</code> block is simply a skip connection as introduced in the ResNet papers. It has one parameter <code class="language-plaintext highlighter-rouge">from</code> which specifies which layer we attach the skip connection from. The activation seems to always be linear so I didn’t bother parsing it.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/shortcut.png" alt="Shortcut block" /></p><h3 id="yolo-block">YOLO block</h3><p>Lastly, there are three <code class="language-plaintext highlighter-rouge">yolo</code> blocks, which require some explanation on the YOLOv3 architecture itself.</p><p>On a high level, the way YOLO makes predictions is by first dividing up the image into equally-sized grids, where each grid is responsible for making generating <code class="language-plaintext highlighter-rouge">B</code> bounding box predictions (i.e. B objects, where B = 3 in our version of YOLOv3). The prediction for each grid has depth <code class="language-plaintext highlighter-rouge">B x (5 + C)</code>, because each prediction comes with a bounding box of format <code class="language-plaintext highlighter-rouge">x_center, y_center, width, height</code> and an objectness score, which is the predicted probability that there is indeed an object in the bounding box (4 + 1 = 5). C then represents the number of classes, and the C depth is simply the confidences for each class in that bounding box.</p><p>Now, this process of divide-into-grid-and-predict is actually done three times in our network definition, corresponding to the three [yolo] blocks. So we can think of these three <code class="language-plaintext highlighter-rouge">yolo</code> block outputs as object bounding box predictions done on three different scales, so that the network doesn’t miss out on really large or really small objects.</p><p>But this brings a catch when corresponding the bounding box <code class="language-plaintext highlighter-rouge">xywh</code> predictions to actual locations in the image. The <code class="language-plaintext highlighter-rouge">xywh</code> don’t actually correspond to the actual sizes in the original image, since we downsample the images at different scales. In this case, YOLO uses something called anchors, which is just a fancy way of saying that whatever the <code class="language-plaintext highlighter-rouge">xy</code> output, we are taking the sigmoid of the output (which normalizes the coordinates to within the grid) and then add the output onto the top left corner of the grid’s original image coordinate. The output width and height are also normalized by p_w and p_h, which are the width and height anchor dimensions of the box respectively. Below is a good visual depiction of this transform, which I plagiarized from the YOLOv3 paper, which plagiarized it from someone else.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/transforms.png" alt="Transforms" /></p><p>Now back to our implementation, what we want to do then is simply apply the transformations I previously mentioned onto the current layer, and output them as detections.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/yolo_transform.png" alt="YOLO transforms implementation" /></p><p>An odd thing I didn’t really understand from the config file is that there are two parameters defined, mask and anchors. Anchors is a long list of tuples, and the anchors we want to actually use are the ones corresponding to the indices specified by mask.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/yolo_parse.png" alt="YOLO whack" /></p><p>Leaving that detail aside, we now get three detection outputs done on three different scales. I then concatenate these outputs as one tensor of shape [<code class="language-plaintext highlighter-rouge">number of predictions</code>, <code class="language-plaintext highlighter-rouge">B x (5 + C)</code>] as the final output.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/yolo_return.png" alt="YOLO return" /></p><p>I hope it’s clear how we are formulating the network so far, because we are done with the actual YOLO architecture itself!</p><h2 id="loading-weights">Loading weights</h2><p>The way I tested whether my architecture had no major problems was to first feed it an image. Of course, the outputs would be meaningless since all the model weights are randomly initialized, but it was a nice check that the model worked smoothly and the returned shape is as we expected.</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>darknet = DarkNet()
output = darknet.build(img) # check the outputs
</pre></table></code></div></div><p>Now, to validate that our implementation actually works, it would be nice to set up an inference pipeline, and actually run the model on some test images. Thankfully, the author Joseph Redmon also included the weights they used in his <a href="https://pjreddie.com/darknet/yolo/">webpage</a>. So all we need to do is to load the weight file and check that our inferences look somewhat decent.</p><p>I did meet an issue at this point, though. The above is what I thought would happen in practice: I would build the model, then return the outputs in the format we specified above. However, I never considered how we might load the weights into the model in the first place — I just assumed that we can later set up a function like <code class="language-plaintext highlighter-rouge">model.load_weights()</code> and pass in the weights file. There were two issues I need to figure out: firstly, Tensorflow seems to only only support loading weights from either a ckpt file or a frozen graph. So my first issue was to find out how to convert the downloaded weight file into one of those two formats. Secondly, at least to my understanding, the way I use a Tensorflow model after loading the weight seems to be running a session, then getting a tensor whose name is defined in the ckpt/frozen graph file (which looked quite ugly to me — if I knew this earlier, I would probably design the YOLO class a bit differently. I might make some architecture revisions before posting the code on GitHub though).</p><h3 id="converting-weights-to-checkpoints">Converting weights to checkpoints</h3><p>For the first problem, I kinda stole code (oops) from this tutorial <a href="https://itnext.io/implementing-yolo-v3-in-tensorflow-tf-slim-c3c55ff59dbe">here</a> to convert the model weights file to the Tensorflow ckpt format. It seems that the author uses an older version of Tensorflow, though, but I solved that by importing <code class="language-plaintext highlighter-rouge">import tensorflow.compat.v1 as tf</code> and setting <code class="language-plaintext highlighter-rouge">tf.disable_v2_behavior()</code>. The tutorial also goes over the format of the provided weight file, but essentially all we needed to do was parse the weight file into loading ops, which we can then pass into a Tensorflow saver to convert into a bunch of checkpoint files. I probably won’t go over the details in this blog (since I was essentially just following along the tutorial), but do check it out <a href="https://itnext.io/implementing-yolo-v3-in-tensorflow-tf-slim-c3c55ff59dbe">here</a>!</p><h3 id="tensorflow-sessions-and-tensors">Tensorflow sessions and tensors</h3><p>For my second problem, rather than redesigning my YOLO class, I just defined a new tensor that is the identity of our current output — as you can probably tell, at this point I was getting kinda impatient to see some results, and started slacking off with my code design. It does work and I managed to get the output tensor by its name, but I like to think I paid the price of my dignity as a coder, and would definitely need to update my software structure (and this blog) in the future.</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/identity.png" alt="Identity naming" /></p><p>Nevertheless, at this point, we have fully defined YOLOv3, loaded with the weights that the authors themselves used. Given an input image, we can get the model outputs like so:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/load_outputs.png" alt="Load outputs" /></p><p>Just a tip, if you’re unsure of what your tensor name may be, you can always list all the names in your graph by checking</p><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>[n.name for n in tf.get_default_graph().as_graph_def().node]
</pre></table></code></div></div><h2 id="post-processing">Post-processing</h2><p>But we still need to parse the results into a comprehensible format. So we still need two steps: firstly, since I’m going to be using <code class="language-plaintext highlighter-rouge">cv2.rectangle</code> for drawing my bounding boxes, I wanted to convert YOLOv3’s <code class="language-plaintext highlighter-rouge">xywh</code> output format into a typical bounding box’s top left-bottom right coordinate format. That’s quite simple:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/get_bb.png" alt="Get bounding boxes" /></p><h3 id="non-maximum-suppression">Non-maximum suppression</h3><p>And secondly, we want to apply non-maximum suppression to the output. Why is that so? Recall that how YOLO works is that each grid makes B bounding box predictions, for each anchor defined. This basically generates a ton of predictions, most of which overlap on the same object (assuming the number of objects in the image are not too numerous). So we simply use NMS to sort through the overlapping bounding boxes and find the tightest one. Truth be told, since NMS is quite straightforward and I was getting really impatient, I yet again stole the NMS function of <a href="https://github.com/mystic123/tensorflow-yolo-v3/blob/master/utils.py">another YOLO implementation</a>.</p><p>And we’re done! Now we have the filtered bounding boxes for the predictions YOLOv3 made, along with the predicted classes and confidence on those objects. All that’s left is to draw this onto the original image (or for me, I simply overlay the predictions of the transformed image before transforming it back into its original size). The output is shown below:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/test.png" alt="Get bounding boxes" /></p><h2 id="remarks">Remarks</h2><p>This was quite an exciting project for me, honestly, since I was more focused on the entire learning process rather than showing results, in comparison to schoolwork or internships. Unlike school and internships, I also didn’t have the human resource to refer to, such as upperclassmen, TAs, or mentors, which made it all the more rewarding. Moving forward, I definitely want to work on implementing other SOTA object detection models (particularly Mask-RCNN), but also some human-object pose estimation papers I’ve been quite interested in reading recently. I’ll probably talk more about that in a different post though!</p><h3 id="a-note-on-training">A note on training</h3><p>As a side note, since this post was focused on implementing the YOLOv3 architecture, I slightly brushed over details on setting up the inference pipeline, and didn’t actually implement the training pipeline or the loss function. For those who are curious, here’s a screenshot of the loss function defined in the paper:</p><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/yolo/loss.png" alt="Loss function" /></p><p>where <code class="language-plaintext highlighter-rouge">1_{i}</code> denotes if the object appears in cell <code class="language-plaintext highlighter-rouge">i</code>, and <code class="language-plaintext highlighter-rouge">1_{ij}</code> denotes if the <code class="language-plaintext highlighter-rouge">j</code>th bounding box predictor of cell <code class="language-plaintext highlighter-rouge">i</code> is responsible for that prediction. It is essentially the sum of the SSE for classification loss, localization loss (difference between predicted bounding box and ground truth box), and confidence loss.</p><p>One thing that has been bugging me is that if I were to implement more models in the future, it’d be great if I could come up with a framework for model training, evaluation, and inference, that would ideally be extensible across various kinds of models. That way, in future blog posts, I can just focus on defining model architectures, and have a standardized input and output abstraction ready for running trainings and inference. I don’t really have a solid idea of how that would look like yet, though, but hopefully I’ll have a better idea once I start implementing more models and understand where to draw that abstraction. Till then, thanks for reading!</p></div><div class="post-tail-wrapper text-muted"><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/yolo/" class="post-tag no-text-decoration" >yolo</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><!-- Post sharing snippet v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://www.facebook.com/sharer/sharer.php?title=Implementing YOLOv3 in Tensorflow - Jia Jiunn Ang&u=https://jiajiunn.github.io/posts/yolo/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://twitter.com/intent/tweet?text=Implementing YOLOv3 in Tensorflow - Jia Jiunn Ang&url=https://jiajiunn.github.io/posts/yolo/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://telegram.me/share?text=Implementing YOLOv3 in Tensorflow - Jia Jiunn Ang&url=https://jiajiunn.github.io/posts/yolo/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><!-- The Pannel on right side (Desktop views) v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"><h3 data-toc-skip>Recent Update</h3><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/phosa/">Improving 3D Human-Object Spatial Arrangement Predictions using Affordance -- Project Ideas</a></li><li><a href="/posts/datasets/">Tensorflow and Pytorch Torchvision Datasets</a></li><li><a href="/posts/pipeline/">Software Structure for a Machine Learning Pipeline in PyTorch</a></li><li><a href="/posts/yolo/">Implementing YOLOv3 in Tensorflow</a></li></ul></div><div id="access-tags"><h3 data-toc-skip>Trending Tags</h3><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/mask-rcnn/">mask rcnn</a> <a class="post-tag" href="/tags/yolo/">yolo</a> <a class="post-tag" href="/tags/pose-estimation/">pose estimation</a> <a class="post-tag" href="/tags/pipeline/">pipeline</a> <a class="post-tag" href="/tags/datasets/">datasets</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><h3 data-toc-skip class="pl-3 pt-2 mb-2">Contents</h3><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"> <!-- Recommend the other 3 posts according to the tags and categories of the current post, if the number is not enough, use the other latest posts to supplement. v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2019 Cotes Chung Published under the MIT License --><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/phosa/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Sep 22 <i class="unloaded">2020-09-22T21:08:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Improving 3D Human-Object Spatial Arrangement Predictions using Affordance -- Project Ideas</h3><div class="text-muted small"><p> Recently I’ve been quite intrigued by the area of pose estimation, and read quite a few papers on the subject. I thought I’d take this opportunity to share some of what I learned. I was particularl...</p></div></div></a></div><div class="card"> <a href="/posts/datasets/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Sep 13 <i class="unloaded">2020-09-13T00:20:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Tensorflow and Pytorch Torchvision Datasets</h3><div class="text-muted small"><p> An integral component of any high-performing model is the dataset; it defines the real world examples that form the basis of what a model learns. But for some, gathering a clean, well-formatted dat...</p></div></div></a></div><div class="card"> <a href="/posts/pipeline/"><div class="card-body"> <!-- Date format snippet v2.4.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --> <span class="timeago small" > Sep 11 <i class="unloaded">2020-09-11T02:01:00-04:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Software Structure for a Machine Learning Pipeline in PyTorch</h3><div class="text-muted small"><p> Recently, I’ve been working on designing an end-to-end machine learning model pipeline for a project I am involved in. I just thought I’d share some of my high-level designs here, as a way of summa...</p></div></div></a></div></div></div><!-- Navigation buttons at the bottom of the post. v2.1 https://github.com/cotes2020/jekyll-theme-chirpy © 2020 Cotes Chung MIT License --><div class="post-navigation d-flex justify-content-between"> <a href="/posts/intro/" class="btn btn-outline-primary"><p>Hello World</p></a> <a href="/posts/rcnn-fam/" class="btn btn-outline-primary"><p>A (Really) Brief Overview of the RCNN Family</p></a></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script> <!-- The Footer v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2020 <a href="https://github.com/JiaJiunn">Jia Jiunn Ang</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/">Chirpy</a> theme.</p></div></div></footer></div><!-- The Search results v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/mask-rcnn/">mask rcnn</a> <a class="post-tag" href="/tags/yolo/">yolo</a> <a class="post-tag" href="/tags/pose-estimation/">pose estimation</a> <a class="post-tag" href="/tags/pipeline/">pipeline</a> <a class="post-tag" href="/tags/datasets/">datasets</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <!-- The GA snippet v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script> (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,'script','https://www.google-analytics.com/analytics.js','ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script> <!-- Jekyll Simple Search loader v2.0 https://github.com/cotes2020/jekyll-theme-chirpy © 2017-2019 Cotes Chung MIT License --> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://jiajiunn.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
