---
title: A (Really) Brief Overview of the RCNN Family
date: 2020-08-29 13:09:00 -0400
categories: []
tags: [mask-rcnn]     # TAG names should always be lowercase
---

Lately I’ve been reading up on Mask-RCNN and figuring out how to best implement it, but in the process, I realized that going through the development history of the RCNN family actually gives a fairly intuitive introduction to how the Mask-RCNN structure came to be. This post is by no means an in-depth explanation of how to implement each model — rather, I thought I’d take this chance to go over the highlights of each member of the RCNN family, and hopefully give intuition on how their components slowly evolved and improved into Mask-RCNN, which is a popular solution today for the panoptic segmentation problem. For a more detailed explanation, here are some helpful links: [[1]](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) and [[2]](https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4).

## R-CNN

Some time ago, a team at UC Berkeley addressed the question: given an image, how do we output bounding boxes around objects in the image, along with their corresponding classes? 

At the time, SVMs were a known tool for classifying images, and is still currently quite a popular algorithm for classifying the MNIST dataset in many introduction-to-ML Medium articles. Using this, the team came up with a fairly intuitive solution: generate a bunch of region proposals, then pipe them into an SVM classifier to get the class of potential objects in these proposed regions. It additionally runs a linear regression model after the classes are generated, to tighten the bounding boxes on objects of the respective classes. They called this model the R-CNN, and published a [paper](https://arxiv.org/pdf/1311.2524.pdf) on it.

## Fast R-CNN

After publishing their RCNN paper, the authors sat back, and realized that the model they designed had a major bottleneck: it was taking a ton (~2000) of sub-regions of the same image, most of which overlap, and passing them through a convolutional network. This led to a lot of redundant, as well as time-consuming work — 2000 convolutions take quite some time after all, around 47 seconds — so why can’t they first run convolutions on the entire image, then get region proposals from the feature maps? That’s exactly what they did, which drastically reduced the run time. They then used a technique called ROIPooling to warp the images into a fixed size, then piped the output to a softmax layer and linear regression layer separately. Here’s a nice visual from their paper:

![Fast R-CNN](/assets/rcnn_fam/fast_rcnn_paper.png)

Notice that there is another factor at play here that improves the R-CNN model; previously, R-CNN was comprised of three separate networks: the CNN (for extracting features), the SVM (for identifying object class), and the linear regressor (for tightening the generated bounding boxes). This makes the network somewhat complicated to train - but now, the entire framework is unified under one pipeline, since the SVM is replaced by a simple softmax layer added on top of the CNN, while the linear regression layer is just added as an additional layer parallel to the softmax layer. This way, training can be done in a much simpler fashion, possibly achieving lower accuracy too. The authors call this model [Fast R-CNN](https://arxiv.org/pdf/1504.08083.pdf).

Notice that at this point, inference time has been brought down to 2.3 seconds; however, from this graphic I stole from (credits to) [this Medium article](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e):

![Fast R-CNN time](/assets/rcnn_fam/fast_rcnn_time.png)

we can see that the region proposals have now become the bottlenecks for the model inference, slowing performance down by more than seven-fold.

## Faster R-CNN

You may now be curious: how did we actually generate region proposals since the very beginning? Well, for both R-CNN and Fast R-CNN, we have been using a method called selective search, which essentially generates a bunch of sub-segmentations, then greedily combine similar regions before outputting them. However, this algorithm is not only time-consuming, but also can’t really be optimized much since it doesn’t have parameters to learn. As such, a team at Microsoft Research replaced it with another neural network, which seems to solve most of our problems these days (neural networks are essentially giant approximation algorithms anyway). They called this network the Region Proposal Network (RPN), which, as you can guess, predicts region proposals. The outputs are then reshaped to a fixed size using ROIPooling, and the rest of the network is similar to that of Fast R-CNN’s. Following the naming pattern, they called their improved model [Faster R-CNN](https://arxiv.org/pdf/1506.01497.pdf), since it was, indeed, much faster. Graphics credits yet again from [the same Medium article](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e).

![Faster R-CNN time](/assets/rcnn_fam/faster_rcnn_time.png)

## Mask R-CNN

More recently, a team at Facebook AI Research wondered if they could expand upon this network to generate panoptic masks. Well, you may think, the RPN currently outputs a bunch of region proposals, right? So in addition to sending these regions for classification and drawing bounding boxes and such, can we just send the same regions to another FCN for predicting masks?

If that is indeed what you are thinking, then yes, that’s exactly what those researchers did. They added a branch extending from Fast R-CNN’s RPN, which passes through a standard `CNN + FCN` combo to output a binary mask, where each pixel is classified if it is part of a particular instance of an object. Intuitive, right?

Here's a visual from their [paper](https://arxiv.org/pdf/1703.06870.pdf), which extends from Faster R-CNN with a ResNet backbone:

![Mask R-CNN](/assets/rcnn_fam/mask_rcnn_paper.png)

There is just one last detail to cover; the original region proposals obtained from ROIPool are, as the name suggests, pooled, so there exists a loss of pixel-level precision when we try to map the predicted mask to the original image’s pixel. You can somewhat envision the problem from this example: the 15th pixel of a 128 x 128 image, when pooled down to size 25 x 25, would correspond to the 2.93rd pixel. But since we want to classify each pixel in the original image, the rounding from ROIPool obviously poses a problem.

In response to this, the Facebook researchers came up with ROIAlign: the idea is that instead of rounding our floating points to ints during ROIPool, we apply bilinear interpolation to figure out what values are present in each feature map grid. In essence, we preserve the precision of each pixel-level correspondence, preventing any misalignments between the feature maps and the original image.

## Remarks

And that’s about it! Hopefully that provided you with some insight as to why each component of Mask R-CNN is arranged the way it is -- we put the convolution block first because if it were the other way round, the repeated convolutions would slow down the entire model; the RPN is meant as a substitute for the intuitive act of generating potential regions, and the classifier and bounding boxes are attached to the end of our network for a more unified training process, etc.