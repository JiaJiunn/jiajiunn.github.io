---
title: Mask R-CNN in Keras, Part 1&#58; Backbone and RPN
date: 2020-08-30 21:35:00 -0400
categories: []
tags: [mask-rcnn]     # TAG names should always be lowercase
---

In my previous post, I went through a somewhat high level overview of the R-CNN family, which personally gave me a fairly intuitive idea of each of [Mask R-CNN](https://arxiv.org/pdf/1703.06870.pdf)’s components. Nevertheless, to get a deeper understanding on the network, I thought it’d be a fun project to go through an actual implementation of the paper itself.

## Foreword

What I had in mind was to write an actual implementation of Mask R-CNN from scratch, and be able to run it on example images and get the corresponding predicted masks, bounding boxes and class probabilities. This, however, poses a limitation, since I don’t really plan to set up a training pipeline and spend hours training a decent performing model — hence, I resorted to finding model weights from other implementations online, and tried not to deviate too much from their implementation architecture (i.e. if they use a ResNet101 backbone, I would need to follow suit). 

In this post, I am using matterport’s publicly-available weights, whose implementation can be found [here](https://github.com/matterport/Mask_RCNN). I ended up referencing their implementation a lot, since I had to match the architecture up to their tensor names too, for the weights to be loaded in correctly. Nevertheless, in this post I will try to focus on explaining what I learned — since that is ultimately my goal — but do note that a fair bit of code is not from my own implementation ideas, but [matterport’s](https://github.com/matterport/Mask_RCNN).

## Keras Implementation and Loading Weights

To give an overview of how the Mask R-CNN model is structured, we will be using [Keras’ Model API](https://keras.io/api/models/model/). How the entire model and most of the component will be written is through an API like below:

```
inputs = tf.keras.layers.Input(shape=…, name=“…”)
outputs = process_inputs(inputs)
model = tf.keras.models.Model(inputs, outputs, name=“…”) 
```

We can then use the model to run predictions by calling

```
image = cv2.imread(“…”)  # if the input is an image
outputs = model.predict(image)
```

But most importantly, how do we load in our weights? Well, matterport’s weights are saved in a H5 file format, so we can use the Keras API for parsing the file as such:

```
import h5py
from tensorflow.python.keras.saving import hdf5_format
f = h5py.File(file_name, mode='r')
hdf5_format.load_weights_from_hdf5_group_by_name(f, model.layers)
```

where `model.layers` will give all the layers within the model by name, to be loaded with their corresponding weights. Now that we understand this, we will construct the entire Mask R-CNN architecture as a huge Keras Model object, and load in the weights later as in the example above. Let’s get started!

## ResNet/FPN Backbone

If you recall from the previous post, we first extract features from the input image with a CNN. For this purpose, two popular choices are ResNet and VGG, with ResNet seemingly being the preferred choice as it has skip connections and batch normalization components, which weren’t invented during VGG’s time. There are way more details [here](https://stats.stackexchange.com/questions/280179/why-is-resnet-faster-than-vgg), but we are going to implement a 101-layer [ResNet](https://arxiv.org/pdf/1512.03385.pdf) today since it is the architecture used by our stolen weights. In addition, from the [Mask R-CNN paper](https://arxiv.org/pdf/1703.06870.pdf), the authors found that using a feature pyramid network (FPN) was more effective in capturing features of different scales; hence, we are incorporating this at the end of our ResNet backbone too. 

### ResNet

ResNet is a single convolutional network. At its core, it is comprised of only two types of blocks: the basic block, which has two 3x3 convolutions, and the bottleneck block, which has a 1x1 convolution for dimension control, a 3x3 convolution, and a 1x1 convolution yet again as an upsampling layer. For a more concrete example, take a look at this figure from the [ResNet](https://arxiv.org/pdf/1512.03385.pdf) paper:

![ResNet arch](/assets/mask_rcnn_1/resnet_arch.png)

The layers wrapped in `[]`s are what I refer to as blocks. We can see that the basic block is used exclusively in ResNet18 and ResNet34, while the bottleneck block is used by all the larger ResNet variations. From what I read, one reason for this is that running convolutions on larger feature map inputs tend to be time-consuming, hence bottleneck blocks serve to reduce the size of feature maps before performing convolutions, then upsampling them again before returning the output. 

Regardless of block type, though, the most important aspect of ResNets is what is known as the skip connection around each block. Before the introduction of ResNet, networks with large number of layers are discouraged, since this led to more multiplications during backprop, which usually reduce the gradients exponentially to 0. The introduction of skip connections by the ResNet paper serves to solve this through an identity function from the very start of each block, which is added to the block output; if the identity mapping is optimal, the network then can easily take the identity function and push the residual to 0, rather than forcefully fitting the identity mapping to a stack of non-linear convolution layers.

In summary, here’s what a block looks like in code:

![Identity block](/assets/mask_rcnn_1/identity_block.png)

where `input_tensor` is the identity mapping that “skips” to the end of the block.

Now, we are only missing one last concept about ResNets: downsampling. Notice that on the left side of the table above, there is a list of layer names. With the exception of the first and second layers, every other layer is preceded by a downsampling during the first convolution. That is to say, during the first block of each of `conv3_x`, `conv4_x`, and `conv5_x`, the convolution first convolution has a stride of 2 to correspond to a downsample. In response, the skip connection also should have a convolution with the same amount of stride, as follows.

![Convolution block](/assets/mask_rcnn_1/convolution_block.png)

Now, all we need to do is assemble the pieces together. For easy visualization, here’s also a screenshot of matterport’s implementation: you can see that aside from the first layer, each `conv_x` layer is preceded with a convolutional-type block before the identity-type blocks. Also notice how for the second layer’s convolution-type block, the stride is 1 instead of 2, as we mentioned above. We then output the feature maps from each convolution layer into our feature pyramid network.

![ResNet code](/assets/mask_rcnn_1/resnet_code.png)

### Feature Pyramid Network

An FPN, as the name suggests, essentially forms a pyramid of feature maps at different scales. If you notice from the our modified ResNet before, we returned each convolution layer’s output as opposed to only the final layer’s. The intuition behind this is when we only choose one feature map, for instance the final feature map of our ResNet output, C5, we are committing ourselves to just one particular scale and resolution for our detected objects. At the scale of C5, we would miss out of plenty of smaller objects, since its resolution is really small. 

However, the reason we conventionally take the C5 output is that as the number of convolutions increase, from C2 to C3 to C5, the semantic value learnedΩ in each feature map increases, which is what we look for in detecting objects and their classes. The initial feature maps like C2 and C3, although high in resolution, simply have not learned enough semantic meaning to justify using them for inference. To counter this problem, though, all we have to do is take the semantic value learned from the last feature map, and combine it with our higher-resolution feature maps, to capture the semantic value of smaller objects. Here’s a [really good explanation](https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c) on FPNs and how they work in an RPN, from which I stole the following graphic:

![FPN graphic](/assets/mask_rcnn_1/fpn_graphic.png)

As you can see, by obtaining feature maps at various scales, adding semantic values, then running inference on them, we can then capture a larger variety of objects at different sizes. Here’s the implementation with corresponding variable names for better reference:

![FPN implementation](/assets/mask_rcnn_1/fpn_implementation.png)

### Region Proposal Network

Finally, we take these feature maps of different resolutions, and use a CNN to predict regions with objects in them. Here’s a description of the outputs of the CNN, taken from the [Faster R-CNN paper](https://arxiv.org/abs/1506.01497) where RPNs were introduced:

![RPN graphic](/assets/mask_rcnn_1/rpn_graphic.png)

where k is the number of anchor boxes. As I described in a previous post, anchor boxes are essentially aspect ratios of the width and height which we commonly expect objects to be close to. Typically, we use these three aspect ratios: `[0.5, 1, 2]`. On the left of the figure, the 2 numbers for each class prediction is the score of how likely that object is a foreground object and a background object respectively. On the right, the 4 numbers simply defines the bounding box of the object at that anchor. Below is the Python implementation:

![RPN implementation](/assets/mask_rcnn_1/rpn_implementation.png)

We take the softmax of the class score output to normalize them into the class probabilities. Notice that in the implementation above, by setting the stride of the shared convolution to the default value of 1, we are creating anchor boxes for each grid in the feature map. We can vary this too, for instance, by setting its stride to be 2, which creates anchor boxes at every other grid.

And now, we are done with the RPN stage! We have just generated a ton of bounding box predictions, along with the predicted foreground/background class probabilities for each bounding box. To test this out, I hacked together a script based on matterport’s [model analysis code](https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/inspect_model.ipynb), that displays our predicted bounding boxes. Essentially, after outputting the RPN class probabilities, I sorted them and found the 100 highest foreground class probabilities. I then found the corresponding bounding boxes and overlayed them onto the image.

![Visualize script](/assets/mask_rcnn_1/visualize_script.png)

And finally, here are some output images!

![image_1](/assets/mask_rcnn_1//sample_output_1.png) 
![image_2](/assets/mask_rcnn_1/sample_output_2.png)
![image_3](/assets/mask_rcnn_1/sample_output_3.png)

## Next Stages

Notice that the image has been transformed into a square format. Ideally, we would also crop the bounding boxes that have exceeded the image boundaries, as well as apply non-maximum suppression to reduce the number of overlapping bounding boxes. I’ll include that in a different post, though, as well as implementations and explanations for the rest of the network. 

For now, I hope the first part of the model makes sense to you idea-wise -- we now have a working framework for obtaining region proposals, which potentionally contain objects. All we need to do now is transform the regions into a standardized size, and feed it into (in parallel):
1. a classifier for predicting object class
2. a second classifier for inferring binary masks (whether a pixel is part of an object or not)
3. a regressor for tightening the bounding box