---
title: Improving 3D Human-Object Spatial Arrangement Predictions using Affordance (Project Idea?)
date: 2020-09-22 21:08:00 -0400
categories: []
tags: [pose-estimation]     # TAG names should always be lowercase
---

Recently I’ve been quite intrigued by the area of pose estimation, and read quite a few papers on the subject. I thought I’d take this opportunity to share some of what I learned. I was particularly amazed by [this](https://arxiv.org/pdf/2007.15649.pdf) paper, PHOSA, which proposes a novel framework for estimating human object spatial arrangements at the same time. 

Based on this, I also had a few project ideas I thought would be fun to attempt in order to improve this pipeline. Of course, I’m still quite new to the entire field, so my understanding is rather high-level and might not work at all — but I think it’ll be a great learning process over the coming few months, and I’ll try to document it here anyways.

Let’s start by first understanding what the paper is about!

## Human Pose Estimation

For the past few years, lots of work has been done on human pose estimation - given single images from monocular cameras, we are able to quite decently estimate the 3D pose of humans within the images. One of the more popular frameworks is from [this](https://arxiv.org/pdf/1712.06584.pdf) paper on human mesh recovery, proposed by Kanazawa et al.; it first runs the image through 3D feature extractors and then pipes the output into a discriminator, which discriminates whether the predicted pose is logically possible for a normal human being. Here's a visual from the paper:

![hmr network](/assets/phosa/hmr_network.png)

It's purpose is to filter out poses that may although seem to have plausible predictions from the camera's point of view, are actually clearly impossible for humans to achieve, such as those below:

![hmr 1](/assets/phosa/hmr_1.png)

Joo et al. then expanded upon this idea by including 3D priors of human poses into the pipeline:

![hmr 2](/assets/phosa/hmr_2.png)

I haven’t really looked through these papers in detail, but it would definitely be interesting to attempt implementing them in a future post.

## Human-Object Pose Estimation

But, to quote PHOSA's authors, humans aren’t the end of the story. Estimating human poses might be interesting, but it would certainly be more useful in a practical sense to predict these human meshes in the context of its surroundings as well. Such a system could have a wide range of applications, from obtaining visual cues to action monitoring to trajectory predictions in autonomous cars. A problem we face is that most current approaches consider human and object pose estimation separately — and, when integrated separately into a global frame, we get figures like these which do not make sense spatially:

![phosa before](/assets/phosa/phosa_before.png)

Note: all images in the following sections are by PHOSA's authors :)

To counter this problem, we refer to PHOSA, which refines this global pose estimation by taking into account the human object overlaps, typical object and human sizes, as well as typical interactions between humans and different object types. We’d ideally be able to refine the above image into:

![phosa after](/assets/phosa/phosa_after.png)

But before we delve into this correction process, let’s have a look at the object pose estimator PHOSA uses!

## Object Estimation

Object estimation has also come a long way in the past decade. In particular, the object pose estimator that PHOSA’s authors implemented is as follows: given an image, we first segment objects in the image, using frameworks such as Mask R-CNN. We then an instance mask as well as an object class; the object class is then used to select an exemplar mask, which would be fitted onto the instance mask via an iterative process called a 3D differential renderer. In essence, it optimizes the 6 degree of freedoms for the mesh to best match the instance mask produced. 
Additionally, certain objects have multiple meshes to account for variations within the object class itself — for instance, there might be only one mesh for skateboards, but multiple 3D meshes for bikes, since the size and design can vary a lot. Here’s a really great illustration from the PHOSA paper describing the process:

![obj pose](/assets/phosa/obj_pose.gif)

Authors of the paper also note that their implementation uses an occlusion-aware silhouette loss, which is robust to occlusion on the estimated object — as long as they have masks of the object that occludes it. More details can be found in [this](https://arxiv.org/pdf/1912.08193.pdf) PointRend paper by Kirillov et al.; I definitely plan to explore this area more in future posts too.

## PHOSA

So far, we have managed to produce 3D pose estimates of humans and objects separately. 

![phosa 2](/assets/phosa/phosa_2.png)

The PHOSA paper then combines these images into a global coordinate frame, which they call “independent composition” -- but due to depth ambiguity in single images, here’s the typical result:

![phosa before 2](/assets/phosa/phosa_before_2.png)

where again, the independent compositions don't take into account how human-object pairs interact, leading to flawed combined results. This is what w want, though:

![phosa after 2](/assets/phosa/phosa_after_2.png)

But here’s where the fun starts! We now want to refine this estimation using some real world intuition and common sense. PHOSA, as I understand, is composed of two parts:

- Contact region enforcing
- Scale correction

### Contact Region Enforcing

For contact region enforcing, the pipeline labels and classifies different contact regions on the human body, and labels the corresponding contact regions on all objects they have in the dataset. For instance, a human hand would typically be in contact with a bat handle, or the butt would typically touch the surface of a seat, as shown below:

![contact 1](/assets/phosa/contact_1.png)

The authors also included a component called the human-object interaction loss in their loss function, which optimizes for maximizing bounding box overlaps between contact regions. Here's a visual from the paper:

![contact 2](/assets/phosa/contact_2.png)

In practice, the loss also solves the human-object overlap problem we noted previously, as it discourages deviations of the contact regions.

![compare](/assets/phosa/compare.png)

### Scale Correction

Another intuition for human-object pose estimation we humans have is an estimate of how large an object typically is. The paper gives quite a nice illustrative example below:

![scale 1](/assets/phosa/scale_1.png)

Given the single image above, we would expect the surfer to be standing on the surf board, not just based on how the human-object combination typically interact, but also how large we expect a typical surf board to be. Notice that the first and second concepts both correspond to the two integral ideas behind PHOSA.

To encode this idea, authors of the paper translate all objects in the image into an intrinsic scale relative to a human of height 1.7 meters. Using this scale, they then initialized all object classes to their typical sizes as found on the internet. The results can be seen as such:

![scale 2](/assets/phosa/scale_2.png)

After training, here are what the authors found on the distribution of object sizes within their dataset:

![scale results](/assets/phosa/scale_results.png)

### Network and Loss

We now have the following pipeline:

![phosa network](/assets/phosa/phosa_network.png)

To wrap things up, here’s the loss function PHOSA uses:

![phosa loss](/assets/phosa/phosa_loss.png)

We can see that there’s the occlusion-aware silhouette loss used by the object pose estimator; the human-object interaction loss for enforcing contact regions; the scale prior for taking into account variations in object and human sizes (not everyone is 1.7 meters tall unfortunately); ordinal depth loss for refining object order from the front of the image; and interpenetration loss to discourage human-object overlaps.

## Improvement Ideas

From my perspective, PHOSA can be improved in several areas; firstly, its contact region enforcing is rather loose, as it simply defines *where* the human-object pairs interact, rather than *how*. I thought I could somehow integrate the concept of affordances from robotics into the pipeline somehow, using ideas from a paper I’ll introduce later. Next, in terms of scale correction, large variances of size within the object class could potentially mess up the human-object interaction refinement. For instance, the car class could include anything ranging from small Mini Coopers to large SUVs; and when initialized incorrectly, could potentially mess up the assumed interaction completely. Of course, this problem can be easily fixed through using more fine-grained classes. As for the occlusion-aware silhouette loss, we seem to need masks for all object classes in the image, which would be tough for “in the wild” situations, which is the context of this paper. Lastly, the pipeline seems to be very far from real time, as the object pose estimation process itself takes minutes to complete.

## Detecting and Recognizing Human-Object Interactions

Now, on to the proposed solution. We first introduce [this](https://openaccess.thecvf.com/content_cvpr_2018/papers/Gkioxari_Detecting_and_Recognizing_CVPR_2018_paper.pdf) paper by Gkioxari et al., on detecting and recognizing human object interactions. Again, I’ll cover it in more detail in a future post; but on a surface level, this paper proposes a network that detects `<human, verb, object>` triplets from singular images, thereby inferring action and interactions within the image. 

It builds off Faster R-CNN. Much like how Mask R-CNN simply adds a binary mask branch, this network adds two additional branches; the first one is a human-centric branch, which is given all the detected humans within the image. It focuses on classifying actions from those individuals, as well as predicting a target object density as a 4-dimensional Gaussian, where the mean is where we’d expect a target object to be. Examples of this can be seen below, from images I referenced from the paper:

![obj pred](/assets/phosa/obj_pred.png)

The red overlays are the inferred Gaussians of possible object locations based on the human pose. For instance, we would expect the man with arms outstretched and eyes forward to be reaching out for an object exactly at the position the red overlay indicates; that is the overall idea the first branch’s outputs.s

It is also important to note that a person can be doing multiple actions at once (reading and sipping tea, for instance). Hence, the network’s action (and score) output is fed into a binary sigmoid classifier to enable multiple predictions at the same time. Using this output, the second branch then scores the predicted actions based on human-object pairs in the image; it is thus called the interaction recognition branch. 

### Loss

We use a loss comprising of 3 components:
- Classification and regression loss from object detection (Faster R-CNN)
- Action classification and target localization loss from human-centric branch
- Action classification loss from interaction recognition branch

On an unrelated note, the paper measures precision and recall by considering a classification as a true positive only when the human and object detected have an IOU of greater than 0.5, as well as a matching action labelled for the pair.

## Bringing It All Together

Now, back to addressing problems above. We want a pipeline that is ideally simpler than PHOSA’s proposed network; it is currently comprised of three stages: generating human and object pose estimations independently, combining them in a common coordinate frame, then refining the combination. Much like the transition from R-CNN to Fast R-CNN, we want to simplify this network into as few stages as possible, which would potentially make optimizing the pipeline easier in the future.

### Project Idea

Well, we first look at the differential renderer — why are we optimizing the object mask to match the instance mask? The instance mask is but an arbitrary pose estimation in the shared coordinate space; when we eventually combine the human and object poses together, there is still an entire degree of freedom which it can lie anywhere on. Intuitively, I thought that instead of optimizing the pose search on this somewhat arbitrary positioning, we could utilize the 3D human pose to predict potential object locations within the global coordinate space, and then optimize localization to those positions. This would

1. Create a simpler pipeline that is easier to optimize, since we don’t need to generate the object separately in a different coordinate frame then combine it back.
2. Utilizes object affordance and how objects interact rather than just where, which might help improve contact region enforcing.
3. Doesn’t need to account for occlusion masks of all possible classes, since the predicted pose would be in a separate 3D space. 

Here's what I had in mind:

![network proposal](/assets/phosa/proposal.png)

Again, I might be totally wrong about this idea, as I don’t really have much background in the field — but it definitely looks really fun to try implementing this and see where it goes. Any feedback or advice would be really appreciated :) hopefully I’ll give some good updates in the next post!