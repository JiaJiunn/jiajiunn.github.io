---
title: Software Structure for a Machine Learning Pipeline in PyTorch
date: 2020-09-11 02:01:00 -0400
categories: []
tags: [pipeline]     # TAG names should always be lowercase
---

Recently, I’ve been working on designing an end-to-end machine learning model pipeline for a project I am involved in. I just thought I’d share some of my high-level designs here, as a way of summarizing up my ideas and thought process. Here goes!

## Training pipeline

The bulk of the pipeline is, of course, the training portion. My idea was to structure a train file that takes in both a defined model and training dataset, then iterate through the dataset while updating the model weights. 

![train pipeline](/assets/pipeline/train_pipeline.png)

For this purpose, we have two different modules feeding into the train file; for the dataset, we would have a module to load in the dataset, then preprocess the inputs and labels. For an image dataset, this includes modifying the image shape, performing image augmentation, etc., but also to modify the dataset input into a standardized schema before being piped into the train file. Secondly, the model itself would be a subclass of a pre-defined model format — here I call ModelBuilder — which would contain the network definition, loss, as well as optimizer of choice. Finally, within the train file, we would use the standard PyTorch way of iterating through the dataset, feeding inputs into the network and optimizing. Here’s some [sample code](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) from the PyTorch official tutorial, to give some idea of what I’m talking about:

![pt tutorial](/assets/pipeline/pt_tutorial.png)

There’s still one final detail: at regular intervals (as defined in the training config), we want to run several quick evaluation steps to roughly judge the model’s performance. These evaluation steps should only be run on a batch of images from the validation set. Now, we define a third module, here called EvalMetric, which defines a standardized set of metrics for evaluating the model. This is important because we want to use such standardized metrics for comparing performance between models. These metrics would also be defined by another config file, and the outputs will be [recorded on a tensorboard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) for monitoring the model performance across time. Lastly, after training is completed (either because the loss converged or we hit the maximum number of training steps), we would save the frozen graph in a pre-defined directory.

## Evaluation pipeline

For evaluation, what I had in mind was to load the frozen graph, then run predictions across the entire validation dataset. We would still use the same standardized set of metrics I previously mentioned, then generate a report on the model performance. The difference is that contrary to the evaluation steps during our training, which only runs on a dataset the size of our batch, we now run inference across the entire validation set for a model reliable model benchmarking.

![eval pipeline](/assets/pipeline/eval_pipeline.png)

However, one detail I wanted to add was that rather than training every single model from scratch, I wanted a sort of framework for converting weights we potentially find online into a standardized format — which I call here `WeightLoader`. Then, using this standardized format, we can selectively load weights for certain variables, and retrain the rest on our own custom dataset (i.e. transfer learning). In Tensorflow, we would just use `tf.estimator.WarmStartSettings` and define the `vars_to_warm_start`; I’m not too sure how this works with PyTorch models, though, so I might include some details in a future post.

## Prediction pipeline

The prediction pipeline is essentially a much simpler version of evaluation: we load in the weights, but instead of running predictions on the dataset and checking the model performance, all we need to do is feed an arbitrary image and see the output results. I had in mind to write the prediction code in a ROS node-compatible format, but I haven’t looked much into how to do that. Another future post, I guess.

![prediction pipeline](/assets/pipeline/pred_pipeline.png)

## Final words

And here we are! We now have a complete pipeline for training, evaluating, and running predictions for machine learning models. One last detail I had was to use a standardized launch file across all processes, which would also serve useful for launching docker and dockerizing the entire pipeline.

![full pipeline](/assets/pipeline/full_pipeline.png)

Hope you found some of these ideas helpful in designing your own pipeline! Do leave a comment below for any feedback or suggestions on improveming the design.